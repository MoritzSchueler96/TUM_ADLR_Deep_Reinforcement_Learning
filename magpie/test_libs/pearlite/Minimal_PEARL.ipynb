{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal Notebook to train an instance of meta learning Soft Actor Critic (SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import mSAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy, evaluate_meta_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL = 30 #number of scenarios to evaluate model on\n",
    "N_EPOCHS = 3 # number of training epochs\n",
    "N_TIMESTEPS = 2000 # number of Timesteps (=Gradient steps) per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "critic with  15\n",
      "critic with  15\n",
      "critic with  15\n",
      "critic with  15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 9           |\n",
      "|    time_elapsed    | 47          |\n",
      "|    total timesteps | 449         |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.8956873   |\n",
      "|    actor_loss      | 2.74        |\n",
      "|    avg. z          | -0.15789148 |\n",
      "|    avg. z var      | 0.021943886 |\n",
      "|    critic_loss     | 30          |\n",
      "|    ent_coef        | 0.876       |\n",
      "|    ent_coef_loss   | -0.426      |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 448         |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 9           |\n",
      "|    time_elapsed    | 106         |\n",
      "|    total timesteps | 998         |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.90165704  |\n",
      "|    actor_loss      | 5.7         |\n",
      "|    avg. z          | -0.08011629 |\n",
      "|    avg. z var      | 0.013511678 |\n",
      "|    critic_loss     | 13.7        |\n",
      "|    ent_coef        | 0.743       |\n",
      "|    ent_coef_loss   | -0.981      |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 997         |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 9           |\n",
      "|    time_elapsed    | 168         |\n",
      "|    total timesteps | 1671        |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.93916124  |\n",
      "|    actor_loss      | 7.95        |\n",
      "|    avg. z          | -0.09189476 |\n",
      "|    avg. z var      | 0.010661114 |\n",
      "|    critic_loss     | 6.33        |\n",
      "|    ent_coef        | 0.608       |\n",
      "|    ent_coef_loss   | -1.55       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 1670        |\n",
      "------------------------------------\n",
      "epoch: 1\n",
      "meta_reward =  [-126.3643100868635, -347.7509811990122]\n",
      "meta_std =  [50.659850742352816, 103.52804286069244]\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 46          |\n",
      "|    total timesteps | 482         |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.9297407   |\n",
      "|    actor_loss      | 11.9        |\n",
      "|    avg. z          | -0.08841646 |\n",
      "|    avg. z var      | 0.010569105 |\n",
      "|    critic_loss     | 49.4        |\n",
      "|    ent_coef        | 0.48        |\n",
      "|    ent_coef_loss   | -2.26       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 2481        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 83          |\n",
      "|    total timesteps | 892         |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.9270882   |\n",
      "|    actor_loss      | 14.3        |\n",
      "|    avg. z          | -0.10635843 |\n",
      "|    avg. z var      | 0.010830063 |\n",
      "|    critic_loss     | 8.01        |\n",
      "|    ent_coef        | 0.427       |\n",
      "|    ent_coef_loss   | -2.44       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 2891        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 123         |\n",
      "|    total timesteps | 1332        |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.94261134  |\n",
      "|    actor_loss      | 17.7        |\n",
      "|    avg. z          | -0.0986623  |\n",
      "|    avg. z var      | 0.009938466 |\n",
      "|    critic_loss     | 51.2        |\n",
      "|    ent_coef        | 0.378       |\n",
      "|    ent_coef_loss   | -2.65       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 3331        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 170         |\n",
      "|    total timesteps | 1812        |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.9555956   |\n",
      "|    actor_loss      | 17.9        |\n",
      "|    avg. z          | -0.09541518 |\n",
      "|    avg. z var      | 0.009463158 |\n",
      "|    critic_loss     | 21.8        |\n",
      "|    ent_coef        | 0.331       |\n",
      "|    ent_coef_loss   | -2.79       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 3811        |\n",
      "------------------------------------\n",
      "epoch: 2\n",
      "meta_reward =  [-126.3643100868635, -347.7509811990122, -126.24742915416893]\n",
      "meta_std =  [50.659850742352816, 103.52804286069244, 157.77783718730637]\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 58          |\n",
      "|    total timesteps | 611         |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.9713829   |\n",
      "|    actor_loss      | 26.3        |\n",
      "|    avg. z          | -0.12518674 |\n",
      "|    avg. z var      | 0.009029326 |\n",
      "|    critic_loss     | 14.4        |\n",
      "|    ent_coef        | 0.263       |\n",
      "|    ent_coef_loss   | -3.33       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 4610        |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 10          |\n",
      "|    time_elapsed    | 108         |\n",
      "|    total timesteps | 1145        |\n",
      "| train/             |             |\n",
      "|    KL_loss         | 0.97157633  |\n",
      "|    actor_loss      | 29.8        |\n",
      "|    avg. z          | -0.10540209 |\n",
      "|    avg. z var      | 0.008812609 |\n",
      "|    critic_loss     | 15.4        |\n",
      "|    ent_coef        | 0.226       |\n",
      "|    ent_coef_loss   | -3.83       |\n",
      "|    learning_rate   | 0.0003      |\n",
      "|    n_updates       | 5144        |\n",
      "------------------------------------\n",
      "epoch: 3\n",
      "meta_reward =  [-126.3643100868635, -347.7509811990122, -126.24742915416893, -366.9175019046869]\n",
      "meta_std =  [50.659850742352816, 103.52804286069244, 157.77783718730637, 158.7118428600609]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "meta_model = mSAC('MlpPolicy', env, verbose=1,policy_kwargs=dict(net_arch=[300, 300, 300], latent_dim = 5, hidden_sizes=[200,200,200]))#,learning_rate=0.0006)\n",
    "\n",
    "meta_reward = []\n",
    "meta_std = []\n",
    "\n",
    "meta_model_mean_reward_before, meta_model_std_reward_before = evaluate_meta_policy(meta_model, env, n_eval_episodes=N_EVAL)\n",
    "meta_reward.append(meta_model_mean_reward_before)\n",
    "meta_std.append(meta_model_std_reward_before)\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    \n",
    "    meta_model.learn(total_timesteps=N_TIMESTEPS)\n",
    "    meta_model_mean_reward, meta_model_std_reward = evaluate_meta_policy(meta_model, env, n_eval_episodes=N_EVAL)\n",
    "\n",
    "    meta_reward.append(meta_model_mean_reward)\n",
    "    meta_std.append(meta_model_std_reward)\n",
    "    \n",
    "    print('epoch:', i+1)\n",
    "    print('meta_reward = ', meta_reward)\n",
    "    print('meta_std = ', meta_std)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
